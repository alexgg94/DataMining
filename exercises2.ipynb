{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second set of exercices you will have to develop two Machine Learning procedures. In both cases is mandatory to use Apache Spark 2.x and if you need any necessary library to manage data or to generate features you must use Apache MLlib (DataFrame version). Check the following aspects:\n",
    "\n",
    "Problem 1:\n",
    "\n",
    "Using the dataset, build a Machine Learning procedure to predict the price of houses having neighbourhood variables. The Boston House Price Dataset involves the prediction of a house price in thousands of dollars given details of the house and its neighborhood. You can download the data from here: https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data. More info here: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html.\n",
    "\n",
    "Make sure that your dataset is technically correct\n",
    "Check the consistency of your dataset\n",
    "In this exercise is not mandatory to use Pipelines\n",
    "Split your data into two sets: 80% of the data for training and 20% of the data for testing\n",
    "Provide convenient measures to check how the model is behaving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 attributes in each case of the dataset. They are:\n",
    "\n",
    "CRIM - per capita crime rate by town\n",
    "\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS - proportion of non-retail business acres per town.\n",
    "\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM - average number of rooms per dwelling\n",
    "\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "RAD - index of accessibility to radial highways\n",
    "\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "LSTAT - % lower status of the population\n",
    "\n",
    "MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark \n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u' 0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00',\n",
       " u' 0.02731   0.00   7.070  0  0.4690  6.4210  78.90  4.9671   2  242.0  17.80 396.90   9.14  21.60',\n",
       " u' 0.02729   0.00   7.070  0  0.4690  7.1850  61.10  4.9671   2  242.0  17.80 392.83   4.03  34.70',\n",
       " u' 0.03237   0.00   2.180  0  0.4580  6.9980  45.80  6.0622   3  222.0  18.70 394.63   2.94  33.40',\n",
       " u' 0.06905   0.00   2.180  0  0.4580  7.1470  54.20  6.0622   3  222.0  18.70 396.90   5.33  36.20']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"housing.data\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00632,\n",
       " 18.0,\n",
       " 2.31,\n",
       " 0,\n",
       " 0.538,\n",
       " 6.575,\n",
       " 65.2,\n",
       " 4.09,\n",
       " 1,\n",
       " 2960000.0,\n",
       " 15.3,\n",
       " 396.9,\n",
       " 4.98,\n",
       " 24000.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MakeDataConsistent(line):\n",
    "    for index in range(len(line)):\n",
    "        if(index != 3 and index != 8):\n",
    "            line[index] = float(line[index])\n",
    "        else:\n",
    "            line[index] = int(line[index])\n",
    "        if(index == 9):\n",
    "            line[index] = 10000 * line[index]\n",
    "        elif(index == 13):\n",
    "            line[index] = 1000 * line[index]\n",
    "    return line\n",
    "        \n",
    "\n",
    "housing = sc.textFile(\"housing.data\").map(lambda x: x.split()).map(lambda x : MakeDataConsistent(x))\n",
    "print(type(housing))\n",
    "housing.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---------+-------+------+-----+-------+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|      TAX|PTRATIO|     B|LSTAT|   MEDV|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---------+-------+------+-----+-------+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|2960000.0|   15.3| 396.9| 4.98|24000.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|2420000.0|   17.8| 396.9| 9.14|21600.0|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|2420000.0|   17.8|392.83| 4.03|34700.0|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|2220000.0|   18.7|394.63| 2.94|33400.0|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|2220000.0|   18.7| 396.9| 5.33|36200.0|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|2220000.0|   18.7|394.12| 5.21|28700.0|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|3110000.0|   15.2| 395.6|12.43|22900.0|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|3110000.0|   15.2| 396.9|19.15|27100.0|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|3110000.0|   15.2|386.63|29.93|16500.0|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|3110000.0|   15.2|386.71| 17.1|18900.0|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|3110000.0|   15.2|392.52|20.45|15000.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|3110000.0|   15.2| 396.9|13.27|18900.0|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|3110000.0|   15.2| 390.5|15.71|21700.0|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|3070000.0|   21.0| 396.9| 8.26|20400.0|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|3070000.0|   21.0|380.02|10.26|18200.0|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|3070000.0|   21.0|395.62| 8.47|19900.0|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|3070000.0|   21.0|386.85| 6.58|23100.0|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|3070000.0|   21.0|386.75|14.67|17500.0|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|3070000.0|   21.0|288.99|11.69|20200.0|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|3070000.0|   21.0|390.95|11.28|18200.0|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---------+-------+------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('CRIM', FloatType()),\n",
    "    StructField('ZN', FloatType()),\n",
    "    StructField('INDUS', FloatType()),\n",
    "    StructField('CHAS', IntegerType()),\n",
    "    StructField('NOX', FloatType()),\n",
    "    StructField('RM', FloatType()),\n",
    "    StructField('AGE', FloatType()),\n",
    "    StructField('DIS', FloatType()),\n",
    "    StructField('RAD', IntegerType()),\n",
    "    StructField('TAX', FloatType()),\n",
    "    StructField('PTRATIO', FloatType()),\n",
    "    StructField('B', FloatType()),\n",
    "    StructField('LSTAT', FloatType()),\n",
    "    StructField('MEDV', FloatType())\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(housing, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.006320000160485506, ZN=18.0, INDUS=2.309999942779541, CHAS=0, NOX=0.5379999876022339, RM=6.574999809265137, AGE=65.19999694824219, DIS=4.090000152587891, RAD=1, TAX=2960000.0, PTRATIO=15.300000190734863, B=396.8999938964844, LSTAT=4.980000019073486, MEDV=24000.0, neighbourhood_variables=DenseVector([0.0063, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2, 4.09, 1.0, 2960000.0, 15.3, 396.9, 4.98]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'], outputCol = 'neighbourhood_variables')\n",
    "vhouse_df = vectorAssembler.transform(df)\n",
    "vhouse_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|neighbourhood_variables|   MEDV|\n",
      "+-----------------------+-------+\n",
      "|   [0.00632000016048...|24000.0|\n",
      "|   [0.02731000073254...|21600.0|\n",
      "|   [0.02728999964892...|34700.0|\n",
      "+-----------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vhouse_df = vhouse_df.select(['neighbourhood_variables', 'MEDV'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train, to_test = vhouse_df.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-118.24514205819007,44.5401644332137,-74.77194523615698,3039.3675060963847,-14916.055048104574,3929.1882673212394,-9.869240142757794,-1447.8762290576617,195.74562092143822,-0.0003297656816680554,-896.8656687893629,13.204799949789539,-468.38338410322376]\n",
      "Intercept: 30116.3848057\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'neighbourhood_variables', labelCol='MEDV', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(to_train)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-----------------------+\n",
      "|        prediction|   MEDV|neighbourhood_variables|\n",
      "+------------------+-------+-----------------------+\n",
      "| 32597.78121390356|31600.0|   [0.01432000007480...|\n",
      "|23171.301064468396|33000.0|   [0.01950999908149...|\n",
      "|42448.978840272095|50000.0|   [0.02009000070393...|\n",
      "| 28020.14480641378|25000.0|   [0.02875000052154...|\n",
      "|24874.105772169885|28700.0|   [0.02985000051558...|\n",
      "|31516.822632207026|34900.0|   [0.03150000050663...|\n",
      "| 33167.20604630139|30300.0|   [0.04665999859571...|\n",
      "|30906.417127279237|28700.0|   [0.05302000045776...|\n",
      "|28877.386733444808|24600.0|   [0.05424999818205...|\n",
      "| 34033.92402249715|39800.0|   [0.06588000059127...|\n",
      "+------------------+-------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.794089\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(to_test)\n",
    "lr_predictions.select(\"prediction\",\"MEDV\",\"neighbourhood_variables\").show(10)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MEDV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2:\n",
    "\n",
    "Using the dataset, build a Machine Learning procedure to classify if the return of a SONAR signal is a Rock or a Mine. You have all the data available at: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29. Make sure that you use sonar.all-data dataset. Check the following aspects:\n",
    "\n",
    "Make sure that your dataset is technically correct\n",
    "Check the consistency of your dataset\n",
    "In this exercise is mandatory to use Pipelines\n",
    "Split your data into two sets: 80% of the data for training and 20% of the data for testing\n",
    "Check that the labels in both sets are equaly distributed (hint: this is called stratified sampling)\n",
    "Provide convenient measures to check how the model is behaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
